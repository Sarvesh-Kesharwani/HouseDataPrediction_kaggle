{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# setting_up to hide the unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the dataset\nimport pandas as pd\ndata = pd.read_csv('online_shoppers_intention.csv')\ndata_copy = data.copy()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exploring the first report of the data to diagnose the problems in it\ndisplay(data.info())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# exploratory visualizaion of data to check missing value problem\ndisplay(data.isnull().sum())\nimport missingno as msno\nmsno.matrix(data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replacing value 0 with NAN to mark missing values in 3 columns\n\n# priting no of NAN values in above imputed cols \ndisplay(data)\nfor cols in ['Administrative','Informational','ProductRelated']:\n    print(f'{cols} null values:', data[cols].isnull().sum(), sep = '\\n')\n\n# 0->NAN imputation\nimport numpy as np\nfor cols in ['Administrative','Informational','ProductRelated']:\n    data[cols].replace(0, np.nan, inplace= True)\n\n# priting no of NAN values in above imputed cols\ndisplay(data)\nfor cols in ['Administrative','Informational','ProductRelated']:\n    print(f'{cols} null values:', data[cols].isnull().sum(), sep = '\\n')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for cols in ['Administrative','Informational','ProductRelated']:\n    print(f'Unique values in column {cols} are: ')\n    display(data[cols].unique())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking outlier problem in numerical cols\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\n\nax=plt.subplot(234)\nplt.boxplot(data['Administrative'])\nax.set_title('Administrative')\n\nax=plt.subplot(235)\nplt.boxplot(data['Informational'])\nax.set_title('Informational')\n\nax=plt.subplot(236)\nplt.boxplot(data['ProductRelated'])\nax.set_title('ProductRelated')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imputing null values with median as there are outliers in those num_cols\nfor cols in ['Administrative','Informational','ProductRelated']:\n    median_value = data[cols].median()\n    data[cols] = data[cols].fillna(median_value)\n\n# checking NAN/null values again after imputation \nfor cols in ['Administrative','Informational','ProductRelated']:\n    print('{} null values:'.format(cols), data[cols].isnull().sum(), sep = '\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking null values in each col before marking NAN\nprint('null values in each col before marking NAN: ')\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration']:\n    print('{} null values:'.format(cols), data[cols].isnull().sum(), sep = '\\n')\n    \n# replacing unexpected_values in these num_cols with null/NAN to mark missing values  \nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration']:\n    data[cols].replace(0, np.nan, inplace= True)\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration']:\n    data[cols].replace(-1, np.nan, inplace= True)\n    \n# checking null values in each col after marking NAN\nprint('\\n')\nprint('null values in each col after marking NAN: ')\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration']:\n    print('{} null values:'.format(cols), data[cols].isnull().sum(), sep = '\\n')\n    \n# since these num_cols don't contain outliers, we can impute theme with mean values\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration','BounceRates in %','ExitRates in %']:\n    mean_value = data[cols].mean()\n    data[cols] = data[cols].fillna(mean_value)\n\n# checking null values in each col after imputation\nprint('\\n')\nprint('null values in each col after imputing NAN: ')\nfor cols in ['Administrative_Duration','Informational_Duration','ProductRelated_Duration','BounceRates in %','ExitRates in %']:\n    print('{} null values:'.format(cols), data[cols].isnull().sum(), sep = '\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize = (15, 5))\n#plt.style.use('seaborn-white')\nplt.subplot(131)\nsns.scatterplot(x=\"Administrative\", y=\"Administrative_Duration\",hue=\"Revenue\", data=data)\nplt.subplot(132)\nsns.scatterplot(x=\"Informational\", y=\"Informational_Duration\",hue=\"Revenue\", data=data)\nplt.subplot(133)\nsns.scatterplot(x=\"ProductRelated\", y=\"ProductRelated_Duration\",hue=\"Revenue\", data=data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax1 = plt.subplots(figsize=(10,6))\ncolor = 'tab:green'\n\nax1.set_title('Average Page Value by Month', fontsize=16)\nax1.set_xlabel('Month', fontsize=16)\nax1.set_ylabel('Avg Temp', fontsize=16, color=color)\n\nax2 = sns.barplot(x='Month', y='PageValues', data = data, palette='summer',hue='Revenue')\nax1.tick_params(axis='y')\nax2 = ax1.twinx()\n\ncolor = 'tab:red'\nax2.set_ylabel('Avg Percipitation %', fontsize=16, color=color)\nax2 = sns.lineplot(x='Month', y='PageValues', data = data, sort=False, color=color)\nax2.tick_params(axis='y', color=color)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display(data)\nsns.relplot(x=\"BounceRates in %\", y=\"ExitRates in %\",col=\"Revenue\",hue=\"Revenue\",style=\"Weekend\", data=data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"VisitorType\", y=\"ExitRates in %\",\n                hue=\"Weekend\", col=\"Revenue\",\n                data=data, kind=\"box\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # ! pip install missingno\n# import missingno as msno\n# print(\"Missingno Version : {}\".format(msno.__version__))\n# msno.heatmap(data[:3], figsize=(10,5), fontsize=12)\n# # msno.bar(data.corr(), color=\"dodgerblue\", sort=\"ascending\", figsize=(10,5), fontsize=12)\n# ---------------------------------------------------------------------------------------------missingno.heatmap() didn't work\n\n# Generate a mask for the upper triangle\nimport numpy as np\nmask = np.zeros_like(data.corr(), dtype=np.bool)\n# display(data.corr())\nmask[np.triu_indices_from(mask)] = True\n\n# applying one more mask to hide those corr() cells from corr_matrix for which value is <0.75\ncorr_matrix = data.corr()\ncorr_matrix = corr_matrix >=  0.75\n# display(corr_matrix)\n    \n# Set up the matplotlib figure\nfrom matplotlib import pyplot as plt\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n#cmap = sns.diverging_palette(20, 220, n=400)\n\n# plotting the corr_matrix\nimport seaborn as sns\nax = sns.heatmap(data.corr(), mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .7})\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting the corr_matrix where cell>=0.75\ndisplay('highly correlated features: ')\nimport seaborn as sns\nax = sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .7})\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_customer = data.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# display(data)\nplt.figure(figsize = (15, 10))\nplt.style.use('seaborn-white')\n\nax=plt.subplot(231)\nplt.boxplot(feature_customer['BounceRates in %'])\nax.set_title('BounceRates in %')\n\nax=plt.subplot(232)\nplt.boxplot(feature_customer['ExitRates in %'])\nax.set_title('ExitRates in %')\n\nax=plt.subplot(233)\nplt.boxplot(feature_customer['Administrative_Duration'])\nax.set_title('Administrative_Duration')\n\nax=plt.subplot(234)\nplt.boxplot(feature_customer['Informational_Duration'])\nax.set_title('Informational_Duration')\n\nax=plt.subplot(235)\nplt.boxplot(feature_customer['ProductRelated_Duration'])\nax.set_title('ProductRelated_Duration')\n\nax=plt.subplot(236)\nplt.boxplot(feature_customer['PageValues'])\nax.set_title('PageValues')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# removing outliers from num_cols_having_outliers\nnumerical_features=['BounceRates in %', 'ExitRates in %', 'Administrative_Duration', 'ProductRelated_Duration']\nfor cols in numerical_features:\n    Q1 = feature_customer[cols].quantile(0.25)\n    Q3 = feature_customer[cols].quantile(0.75)\n    IQR = Q3 - Q1     \n    filter = (feature_customer[cols] >= Q1 - 1.5 * IQR) & (feature_customer[cols] <= Q3 + 1.5 * IQR)\n    feature_customer = feature_customer.loc[filter]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# manually encoding num_cols into cat_cols by replacing the values in column with either 0 or 1\nfeature_customer.loc[feature_customer['SpecialDay (probability)'] > 0.4, 'SpecialDay (probability)'] = 1\nfeature_customer.loc[feature_customer['SpecialDay (probability)'] <= 0.4, 'SpecialDay (probability)'] = 0\n\n# as we can see now that the col contain only 0/1 values\ndisplay(feature_customer['SpecialDay (probability)'])\n\n# count of 0s and 1s in the col\ndisplay(feature_customer['SpecialDay (probability)'].value_counts())\n\n# converting 0/1 to bool dtype\nfeature_customer['SpecialDay (probability)'] = feature_customer['SpecialDay (probability)'].astype('bool')\ndisplay(feature_customer['SpecialDay (probability)'].value_counts())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking remaining cat_cols\nfeature_customer.dtypes","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# listing cat_cols\nfrom sklearn.preprocessing import StandardScaler\nCategorical_variables = ['Weekend','Revenue','Administrative','Informational','ProductRelated','SpecialDay',\n                            'OperatingSystems','Browser','Region','Month','TrafficType','VisitorType']\n\n# scaling non_cat_cols(num_cols)\nfeature_scale = [feature for feature in feature_customer.columns if feature not in Categorical_variables]\nscaler = StandardScaler()\nscaler.fit(feature_customer[feature_scale])\n\n# combining scaled_cat_cols and num_cols together\nscaled_data = pd.concat([feature_customer[['Weekend','Revenue','Administrative','Informational',\n                                           'ProductRelated','SpecialDay (probability)','OperatingSystems',\n                                           'Browser','Region','Month','TrafficType','VisitorType']].reset_index(drop=True),\n                        pd.DataFrame(scaler.transform(feature_customer[feature_scale]), columns = feature_scale)],\n                        axis=1)\nscaled_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# only 2 cat_cols are there in dataset\nfrom sklearn.preprocessing import LabelEncoder\nencoded_features = ['Month','VisitorType']\n\n# fitting the LabelEncoder model with all the Cat_features one by one and \n# then encoding each feature with its corrosponding trained_model\nlabel_data = scaled_data.copy()\nlabel_encoder = LabelEncoder()\nfor col in encoded_features:\n    label_data[col] = label_encoder.fit_transform(scaled_data[col])\n\n# printing encoded features\ndisplay(label_data[encoded_features])\ndisplay(label_data.head())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x y split\nfrom sklearn.ensemble import ExtraTreesClassifier\nx = label_data.drop(['Revenue'],axis=1)\nay = label_data.Revenue\n\n# printing importance of 17 most_important features using ExtraTreeClassifier(ETC)\nmodel = ExtraTreesClassifier()\nmodel.fit(x,ay)\nprint(model.feature_importances_)\n\n# plotting first 17 most_important features \nfeat_importances = pd.Series(model.feature_importances_, index=x.columns)\nfeat_importances.nlargest(17).plot(kind='barh')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# dropping un_important columns \n# x y split\nx = label_data.drop(['SpecialDay (probability)','VisitorType','Weekend','Revenue'],axis=1)\nay = label_data.Revenue\n\n# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, ay_train, ay_test = train_test_split(x, ay, train_size=0.8, random_state=1)\nprint(\"Input Training:\",x_train.shape)\nprint(\"Input Test:\",x_test.shape)\nprint(\"Output Training:\",ay_train.shape)\nprint(\"Output Test:\",ay_test.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LR and DT\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nlogreg_cv = LogisticRegression(random_state=0)\ndt_cv = DecisionTreeClassifier()\n\n#  RF and KNN\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nrt_cv = RandomForestClassifier()\nknn_cv = KNeighborsClassifier()\n\n# SVC\nfrom sklearn.svm import SVC\nsvc_cv = SVC(kernel='linear')\n\n# Naive Bays Classifier(NBC)\nfrom sklearn.naive_bayes import BernoulliNB\nnb_cv = BernoulliNB()\n\n# CrossValidation\ncv_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest',3:'KNN',4:'SVC',5:'Naive Bayes'}\ncv_models = [logreg_cv, dt_cv, rt_cv, knn_cv, svc_cv, nb_cv]\n\nfrom sklearn.model_selection import cross_val_score\nfor i,model in enumerate(cv_models):\n    print(\"{} Test Accuracy: {}\".format(cv_dict[i], cross_val_score(model, x, ay, cv=10, scoring ='accuracy').mean()))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating the pipeline\nfrom sklearn.pipeline import Pipeline\npipeline_lr = Pipeline([('lr_classifier',LogisticRegression(random_state=0))])\npipeline_dt = Pipeline([('dt_classifier',DecisionTreeClassifier())])\npipeline_randomforest = Pipeline([('rf_classifier',RandomForestClassifier())])\npipeline_knn = Pipeline([('knn_classifier',KNeighborsClassifier())])\npipeline_svc = Pipeline([('svc_classifier',SVC(kernel='linear'))])\npipeline_nb = Pipeline([('nb_classifier',BernoulliNB())])\n\n# Assigning the pipeline and relevant outcome variable\npipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest, pipeline_knn, pipeline_svc, pipeline_nb]\nbest_accuracy=0.0\nbest_classifier=0\nbest_pipe=\"\"\n\n# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'RandomForest',3:'KNN',4:'SVC',5:'Naive Bayes'}\n\n# Fit the pipelines\nfor pipe in pipelines:\n\tpipe.fit(x_train, ay_train)\n\n# Evaluating each model\nfor i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i], model.score(x_test, ay_test)))\n    \n# Choosing the best model for our problem\nfor i,model in enumerate(pipelines):\n    if model.score(x_test, ay_test)>best_accuracy:\n        best_accuracy = model.score(x_test, ay_test)\n        best_pipe = model\n        best_classifier = i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a pipeline\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline((RandomForestClassifier()))\n\n# Create dictionary with candidate_learning_algorithms and their hyperparameters\ngrid_param = [{\"randomforestclassifier\": [RandomForestClassifier()],\n                 \"randomforestclassifier__n_estimators\": [10, 100, 1000],\n                 \"randomforestclassifier__max_depth\":[5,8,15,25,30,None],\n                 \"randomforestclassifier__min_samples_leaf\":[1,2,5,10,15,100],\n                 \"randomforestclassifier__max_leaf_nodes\": [2, 5,10]}]\n\n# Gridsearch of the pipeline, the fit the best model\nfrom sklearn.model_selection import GridSearchCV\ngridsearch = GridSearchCV(pipe, grid_param, cv=5, verbose=0,n_jobs=-1) \n# Fit grid search\nbest_model = gridsearch.fit(x_train, ay_train)\n\n# prediction using RFC pipeline\nprint(best_model.best_estimator_)\nprint(\"The mean accuracy of the model is:\",best_model.score(x_test, ay_test))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# RFC train, predict, accuracy_check using 1 matrices\nfrom sklearn import metrics\nrt = RandomForestClassifier(max_depth=30, max_leaf_nodes=10,min_samples_leaf=15)\nrt.fit(x_train, ay_train)\ny_pred=rt.predict(x_test)\nprint(\"Accuracy:\", metrics.accuracy_score(ay_test, y_pred))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SVC train, predict, accuracy_check using 8 matrices\nsvc_classifier = SVC(kernel='linear',random_state = 0)\nsvc_classifier.fit(x_train, ay_train)\npy_test=svc_classifier.predict(x_test)\nfrom sklearn.metrics import classification_report\nprint(\"Accuracy:\",metrics.accuracy_score(ay_test, py_test))\nprint(\"Classification Report:\\n\", classification_report(ay_test, py_test))","metadata":{},"execution_count":null,"outputs":[]}]}