{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.20.0\n",
    "! pip install keras_nlp==0.3.0\n",
    "! pip install datasets\n",
    "! pip install huggingface-hub\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Only log error messages\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The percentage of the dataset you want to split as train and test\n",
    "TRAIN_TEST_SPLIT = 0.1\n",
    "\n",
    "MAX_INPUT_LENGTH = 1024  # Maximum length of the input to the model\n",
    "MIN_TARGET_LENGTH = 5  # Minimum length of the output by the model\n",
    "MAX_TARGET_LENGTH = 128  # Maximum length of the output by the model\n",
    "BATCH_SIZE = 8  # Batch-size for training our model\n",
    "LEARNING_RATE = 2e-5  # Learning-rate for training our model\n",
    "MAX_EPOCHS = 1  # Maximum number of epochs we will train the model for\n",
    "\n",
    "# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\n",
    "MODEL_CHECKPOINT = \"t5-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install keras_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The Tensorflow package version needs to be at least 2.11.0 for KerasCV to run. Currently, your TensorFlow version is 2.10.1. Please upgrade with `$ pip install --upgrade tensorflow`. You can use `pip freeze` to check afterwards that everything is ok.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m resnet_v1\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_cv\\__init__.py:18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright 2022 The KerasCV Authors\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m# isort:off\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m version_check\n\u001b[1;32m---> 18\u001b[0m version_check\u001b[39m.\u001b[39;49mcheck_tf_version()\n\u001b[0;32m     19\u001b[0m \u001b[39m# isort:on\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras_cv\u001b[39;00m \u001b[39mimport\u001b[39;00m callbacks\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras_cv\\version_check.py:26\u001b[0m, in \u001b[0;36mcheck_tf_version\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_tf_version\u001b[39m():\n\u001b[0;32m     25\u001b[0m     \u001b[39mif\u001b[39;00m parse(tf\u001b[39m.\u001b[39m__version__) \u001b[39m<\u001b[39m parse(MIN_VERSION):\n\u001b[1;32m---> 26\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m     27\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe Tensorflow package version needs to be at least \u001b[39m\u001b[39m{\u001b[39;00mMIN_VERSION\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfor KerasCV to run. Currently, your TensorFlow version is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtf\u001b[39m.\u001b[39m__version__\u001b[39m}\u001b[39;00m\u001b[39m. Please upgrade with `$ pip install --upgrade tensorflow`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou can use `pip freeze` to check afterwards that everything is ok.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The Tensorflow package version needs to be at least 2.11.0 for KerasCV to run. Currently, your TensorFlow version is 2.10.1. Please upgrade with `$ pip install --upgrade tensorflow`. You can use `pip freeze` to check afterwards that everything is ok."
     ]
    }
   ],
   "source": [
    "from keras_cv.models import resnet_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dd87\n"
     ]
    }
   ],
   "source": [
    "x=99\n",
    "y=44\n",
    "v=87\n",
    "print(\"dd{2}\".format(x,y,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sarve\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (23.0)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0\n",
      "    Can't uninstall 'pip'. No files were found to uninstall.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No metadata found in c:\\users\\sarve\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\n",
      "WARNING: Error parsing requirements for pip: [Errno 2] No such file or directory: 'c:\\\\users\\\\sarve\\\\appdata\\\\local\\\\packages\\\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\\\localcache\\\\local-packages\\\\python310\\\\site-packages\\\\pip-23.0.dist-info\\\\METADATA'\n",
      "    WARNING: No metadata found in c:\\users\\sarve\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\n",
      "  ERROR: Can't roll back pip; was not uninstalled\n",
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\sarve\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python310\\\\site-packages\\\\pip\\\\_vendor\\\\distlib\\\\__init__.py'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pip.exe, pip3.10.exe and pip3.exe are installed in 'C:\\Users\\sarve\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\sarve\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (23.0)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.0\n",
      "    Uninstalling pip-23.0:\n",
      "      Successfully uninstalled pip-23.0\n",
      "Successfully installed pip-23.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==1.10.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1)\n",
      "ERROR: No matching distribution found for torch==1.10.1\n"
     ]
    }
   ],
   "source": [
    "! pip install torch==1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_random_gen_accumulative_additive_additive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "module() takes at most 2 arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mivy\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mMyModel\u001b[39;00m(ivy\u001b[39m.\u001b[39mcli):\n\u001b[0;32m      4\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m      5\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear0 \u001b[39m=\u001b[39m ivy\u001b[39m.\u001b[39mLinear(\u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: module() takes at most 2 arguments (3 given)"
     ]
    }
   ],
   "source": [
    "import ivy\n",
    "\n",
    "class MyModel(ivy.cli):\n",
    "    def __init__(self):\n",
    "        self.linear0 = ivy.Linear(3, 64)\n",
    "        self.linear1 = ivy.Linear(64, 1)\n",
    "        ivy.Module.__init__(self)\n",
    "\n",
    "    def _forward(self, x):\n",
    "        x = ivy.relu(self.linear0(x))\n",
    "        return ivy.sigmoid(self.linear1(x))\n",
    "\n",
    "ivy.set_backend('torch')  # change to any backend!\n",
    "model = MyModel()\n",
    "optimizer = ivy.Adam(1e-4)\n",
    "x_in = ivy.array([1., 2., 3.])\n",
    "target = ivy.array([0.])\n",
    "\n",
    "def loss_fn(v):\n",
    "    out = model(x_in, v=v)\n",
    "    return ivy.mean((out - target)**2)\n",
    "\n",
    "for step in range(100):\n",
    "    loss, grads = ivy.execute_with_gradients(loss_fn, model.v)\n",
    "    model.v = optimizer.step(model.v, grads)\n",
    "    print('step {} loss {}'.format(step, ivy.to_numpy(loss).item()))\n",
    "\n",
    "print('Finished training!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ivy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mivy\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ivy'"
     ]
    }
   ],
   "source": [
    "import ivy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'NoneType' has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m)\u001b[39m.\u001b[39;49mtype)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'NoneType' has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(type(None).type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_frontend_function(\n",
    "    *,\n",
    "    input_dtypes: Union[ivy.Dtype, List[ivy.Dtype]],\n",
    "    test_flags: pf.frontend_function_flags,\n",
    "    on_device=\"cpu\",\n",
    "    frontend: str,\n",
    "    fn_tree: str,\n",
    "    rtol: float = None,\n",
    "    atol: float = 1e-06,\n",
    "    test_values: bool = True,\n",
    "    **all_as_kwargs_np,\n",
    "):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \"\"\"Tests a frontend function for the current backend by comparing the result with\n",
    "    the function in the associated framework.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dtypes\n",
    "        data types of the input arguments in order.\n",
    "    all_aliases\n",
    "        a list of strings containing all aliases for that function\n",
    "        in the current frontend with their full namespaces.\n",
    "    frontend\n",
    "        current frontend (framework).\n",
    "    fn_tree\n",
    "        Path to function in frontend framework namespace.\n",
    "    rtol\n",
    "        relative tolerance value.\n",
    "    atol\n",
    "        absolute tolerance value.\n",
    "    test_values\n",
    "        if True, test for the correctness of the resulting values.\n",
    "    all_as_kwargs_np\n",
    "        input arguments to the function as keyword arguments.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ret\n",
    "        optional, return value from the function\n",
    "    ret_np\n",
    "        optional, return value from the Numpy function\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    assert (\n",
    "        not test_flags.with_out or not test_flags.inplace\n",
    "    ), \"only one of with_out or with_inplace can be set as True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # split the arguments into their positional and keyword components\n",
    "    args_np, kwargs_np = kwargs_to_args_n_kwargs(\n",
    "        num_positional_args=test_flags.num_positional_args, kwargs=all_as_kwargs_np\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # extract all arrays from the arguments and keyword arguments\n",
    "    arg_np_vals, args_idxs, c_arg_vals = _get_nested_np_arrays(args_np)\n",
    "    kwarg_np_vals, kwargs_idxs, c_kwarg_vals = _get_nested_np_arrays(kwargs_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # make all lists equal in length\n",
    "    num_arrays = c_arg_vals + c_kwarg_vals\n",
    "    if len(input_dtypes) < num_arrays:\n",
    "        input_dtypes = [input_dtypes[0] for _ in range(num_arrays)]\n",
    "    if len(test_flags.as_variable) < num_arrays:\n",
    "        test_flags.as_variable = [test_flags.as_variable[0] for _ in range(num_arrays)]\n",
    "    if len(test_flags.native_arrays) < num_arrays:\n",
    "        test_flags.native_arrays = [\n",
    "            test_flags.native_arrays[0] for _ in range(num_arrays)\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # update var flags to be compatible with float dtype and with_out args\n",
    "    test_flags.as_variable = [\n",
    "        v if ivy.is_float_dtype(d) and not test_flags.with_out else False\n",
    "        for v, d in zip(test_flags.as_variable, input_dtypes)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # frontend function\n",
    "    # parse function name and frontend submodules (jax.lax, jax.numpy etc.)\n",
    "    if isinstance(frontend, list):\n",
    "        frontend, frontend_proc = frontend\n",
    "    split_index = fn_tree.rfind(\".\")\n",
    "    frontend_submods, fn_name = fn_tree[:split_index], fn_tree[split_index + 1 :]\n",
    "    function_module = importlib.import_module(frontend_submods)\n",
    "    frontend_fn = getattr(function_module, fn_name)\n",
    "\n",
    "    args, kwargs = create_frontend_args_kwargs(\n",
    "        args_np=args_np,\n",
    "        arg_np_vals=arg_np_vals,\n",
    "        args_idxs=args_idxs,\n",
    "        kwargs_np=kwargs_np,\n",
    "        kwarg_np_vals=kwarg_np_vals,\n",
    "        kwargs_idxs=kwargs_idxs,\n",
    "        input_dtypes=input_dtypes,\n",
    "        test_flags=test_flags,\n",
    "    )\n",
    "    args_ivy, kwargs_ivy = ivy.args_to_ivy(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # check and replace NativeClass object in arguments with ivy counterparts\n",
    "    from ivy_tests.test_ivy.test_frontends.test_numpy import convnumpy\n",
    "\n",
    "    convs = {\"numpy\": convnumpy}\n",
    "\n",
    "    if \"torch\" in available_frameworks:\n",
    "        from ivy_tests.test_ivy.test_frontends.test_torch import convtorch\n",
    "\n",
    "        convs[\"torch\"] = convtorch\n",
    "\n",
    "    if \"tensorflow\" in available_frameworks:\n",
    "        from ivy_tests.test_ivy.test_frontends.test_tensorflow import convtensor\n",
    "\n",
    "        convs[\"tensorflow\"] = convtensor\n",
    "\n",
    "    if \"jax\" in available_frameworks:\n",
    "        from ivy_tests.test_ivy.test_frontends.test_jax import convjax\n",
    "\n",
    "        convs[\"jax\"] = convjax\n",
    "\n",
    "    if frontend.split(\"/\")[0] in convs:\n",
    "        conv = convs[frontend.split(\"/\")[0]]\n",
    "        args = ivy.nested_map(args, fn=conv, include_derived=True)\n",
    "        kwargs = ivy.nested_map(kwargs, fn=conv, include_derived=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Make copy for arguments for functions that might use\n",
    "    # inplace update by default\n",
    "    copy_kwargs = copy.deepcopy(kwargs)\n",
    "    copy_args = copy.deepcopy(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   # strip the decorator to get an Ivy array\n",
    "    # ToDo, fix testing for jax frontend for x32\n",
    "    if frontend.split(\"/\")[0] == \"jax\":\n",
    "        importlib.import_module(\"ivy.functional.frontends.jax\").config.update(\n",
    "            \"jax_enable_x64\", True\n",
    "        )\n",
    "    ret = get_frontend_ret(frontend_fn, *args_ivy, **kwargs_ivy)\n",
    "    if test_flags.with_out:\n",
    "        if not inspect.isclass(ret):\n",
    "            is_ret_tuple = issubclass(ret.__class__, tuple)\n",
    "        else:\n",
    "            is_ret_tuple = issubclass(ret, tuple)\n",
    "        if is_ret_tuple:\n",
    "            ret = ivy.nested_map(\n",
    "                ret,\n",
    "                lambda _x: ivy.array(_x) if not ivy.is_array(_x) else _x,\n",
    "                include_derived=True,\n",
    "            )\n",
    "        elif not ivy.is_array(ret):\n",
    "            ret = ivy.array(ret)\n",
    "        out = ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # pass return value to out argument\n",
    "        # check if passed reference is correctly updated\n",
    "        kwargs[\"out\"] = out\n",
    "        if is_ret_tuple:\n",
    "            flatten_ret = flatten(ret=ret)\n",
    "            flatten_out = flatten(ret=out)\n",
    "            for ret_array, out_array in zip(flatten_ret, flatten_out):\n",
    "                if ivy.native_inplace_support:\n",
    "                    assert ret_array.data is out_array.data\n",
    "                assert ret_array is out_array\n",
    "        else:\n",
    "            if ivy.native_inplace_support:\n",
    "                assert ret.data is out.data\n",
    "            assert ret is out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " elif test_flags.inplace:\n",
    "        assert not isinstance(ret, tuple)\n",
    "        assert ivy.is_array(ret)\n",
    "        if \"inplace\" in list(inspect.signature(frontend_fn).parameters.keys()):\n",
    "            # the function provides optional inplace update\n",
    "            # set inplace update to be True and check\n",
    "            # if returned reference is inputted reference\n",
    "            # and if inputted reference's content is correctly updated\n",
    "            copy_kwargs[\"inplace\"] = True\n",
    "            first_array = ivy.func_wrapper._get_first_array(*copy_args, **copy_kwargs)\n",
    "            ret_ = get_frontend_ret(frontend_fn, *copy_args, **copy_kwargs)\n",
    "            assert first_array is ret_\n",
    "        else:\n",
    "            # the function provides inplace update by default\n",
    "            # check if returned reference is inputted reference\n",
    "            first_array = ivy.func_wrapper._get_first_array(*args, **kwargs)\n",
    "            ret_ = get_frontend_ret(frontend_fn, *args, **kwargs)\n",
    "            assert first_array is ret_\n",
    "            args, kwargs = copy_args, copy_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create NumPy args\n",
    "    args_np = ivy.nested_map(\n",
    "        args_ivy,\n",
    "        lambda x: ivy.to_numpy(x._data) if isinstance(x, ivy.Array) else x,\n",
    "        shallow=False,\n",
    "    )\n",
    "    kwargs_np = ivy.nested_map(\n",
    "        kwargs_ivy,\n",
    "        lambda x: ivy.to_numpy(x._data) if isinstance(x, ivy.Array) else x,\n",
    "        shallow=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " if \"/\" in frontend and not framework_comparator(frontend):\n",
    "        # multiversion zone, changes made in non-multiversion zone should\n",
    "        # be applied here too\n",
    "\n",
    "        try:\n",
    "            # compute the return via the frontend framework\n",
    "            module_name = fn_tree[25 : fn_tree.rfind(\".\")]\n",
    "\n",
    "            pickle_dict = {\"a\": args_np, \"b\": kwargs_np}\n",
    "            process = frontend_proc\n",
    "            z = make_json_pickable(jsonpickle.dumps(pickle_dict))\n",
    "            try:\n",
    "                process.stdin.write(z + \"\\n\")\n",
    "                process.stdin.write(module_name + \"\\n\")\n",
    "                process.stdin.write(fn_name + \"\\n\")\n",
    "                process.stdin.flush()\n",
    "            except Exception as e:\n",
    "                print(\n",
    "                    \"Something bad happened to the subprocess, here are the logs:\\n\\n\"\n",
    "                )\n",
    "                print(process.stdout.readlines())\n",
    "                raise e\n",
    "            frontend_ret = process.stdout.readline()\n",
    "            if frontend_ret:\n",
    "                frontend_ret = jsonpickle.loads(make_json_pickable(frontend_ret))\n",
    "            else:\n",
    "                print(process.stderr.readlines())\n",
    "                raise Exception\n",
    "            if ivy.isscalar(frontend_ret):\n",
    "                frontend_ret_np_flat = [np.asarray(frontend_ret)]\n",
    "            else:\n",
    "                frontend_ret = ivy.to_ivy(frontend_ret)\n",
    "                # tuplify the frontend return\n",
    "                if not isinstance(frontend_ret, tuple):\n",
    "                    frontend_ret = (frontend_ret,)\n",
    "                frontend_ret_idxs = ivy.nested_argwhere(\n",
    "                    frontend_ret,\n",
    "                    lambda x: isinstance(x, np.ndarray) or isinstance(x, ivy.Array),\n",
    "                )\n",
    "                frontend_ret_flat = ivy.multi_index_nest(\n",
    "                    frontend_ret, frontend_ret_idxs\n",
    "                )\n",
    "                frontend_ret_np_flat = [ivy.to_numpy(x) for x in frontend_ret_flat]\n",
    "\n",
    "        except Exception as e:\n",
    "            ivy.unset_backend()\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "        # non-multiversion zone, changes made here should be\n",
    "        # applied to multiversion zone too\n",
    "\n",
    "        # temporarily set frontend framework as backend\n",
    "        ivy.set_backend(frontend.split(\"/\")[0])\n",
    "        try:\n",
    "            # create frontend framework args\n",
    "            args_frontend = ivy.nested_map(\n",
    "                args_np,\n",
    "                lambda x: ivy.native_array(x)\n",
    "                if isinstance(x, np.ndarray)\n",
    "                else ivy.as_native_dtype(x)\n",
    "                if isinstance(x, ivy.Dtype)\n",
    "                else x,\n",
    "                shallow=False,\n",
    "            )\n",
    "            kwargs_frontend = ivy.nested_map(\n",
    "                kwargs_np,\n",
    "                lambda x: ivy.native_array(x) if isinstance(x, np.ndarray) else x,\n",
    "                shallow=False,\n",
    "            )\n",
    "\n",
    "            # change ivy dtypes to native dtypes\n",
    "            if \"dtype\" in kwargs_frontend:\n",
    "                kwargs_frontend[\"dtype\"] = ivy.as_native_dtype(kwargs_frontend[\"dtype\"])\n",
    "\n",
    "            # change ivy device to native devices\n",
    "            if \"device\" in kwargs_frontend:\n",
    "                kwargs_frontend[\"device\"] = ivy.as_native_dev(kwargs_frontend[\"device\"])\n",
    "\n",
    "            # check and replace the NativeClass objects in arguments\n",
    "            # with true counterparts\n",
    "            args_frontend = ivy.nested_map(\n",
    "                args_frontend, fn=convtrue, include_derived=True, max_depth=10\n",
    "            )\n",
    "            kwargs_frontend = ivy.nested_map(\n",
    "                kwargs_frontend, fn=convtrue, include_derived=True, max_depth=10\n",
    "            )\n",
    "\n",
    "            # compute the return via the frontend framework\n",
    "            module_name = fn_tree[25 : fn_tree.rfind(\".\")]\n",
    "            frontend_fw = importlib.import_module(module_name)\n",
    "            frontend_ret = frontend_fw.__dict__[fn_name](\n",
    "                *args_frontend, **kwargs_frontend\n",
    "            )\n",
    "\n",
    "            if ivy.isscalar(frontend_ret):\n",
    "                frontend_ret_np_flat = [np.asarray(frontend_ret)]\n",
    "            else:\n",
    "                # tuplify the frontend return\n",
    "                if not isinstance(frontend_ret, tuple):\n",
    "                    frontend_ret = (frontend_ret,)\n",
    "                frontend_ret_idxs = ivy.nested_argwhere(\n",
    "                    frontend_ret, ivy.is_native_array\n",
    "                )\n",
    "                frontend_ret_flat = ivy.multi_index_nest(\n",
    "                    frontend_ret, frontend_ret_idxs\n",
    "                )\n",
    "                frontend_ret_np_flat = [ivy.to_numpy(x) for x in frontend_ret_flat]\n",
    "            # unset frontend framework from backend\n",
    "            ivy.unset_backend()\n",
    "        except Exception as e:\n",
    "            ivy.unset_backend()\n",
    "            raise e\n",
    "\n",
    "    ret_np_flat = flatten_and_to_np(ret=ret)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # assuming value test will be handled manually in the test function\n",
    "    if not test_values:\n",
    "        return ret, frontend_ret\n",
    "\n",
    "    if isinstance(rtol, dict):\n",
    "        rtol = _get_framework_rtol(rtol, ivy.backend)\n",
    "    if isinstance(atol, dict):\n",
    "        atol = _get_framework_atol(atol, ivy.backend)\n",
    "\n",
    "    value_test(\n",
    "        ret_np_flat=ret_np_flat,\n",
    "        ret_np_from_gt_flat=frontend_ret_np_flat,\n",
    "        rtol=rtol,\n",
    "        atol=atol,\n",
    "        ground_truth_backend=frontend,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2e2420d2af6466ac46d65e7cae12e1763773716a5df9fb9629e3ec3fd0e9e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
